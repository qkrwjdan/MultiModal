{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df0ed588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0027142",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = os.path.join(os.getcwd(),\"datasets\")\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for dataset in os.listdir(datasets_path):\n",
    "    path = os.path.join(datasets_path,dataset)\n",
    "    df = pd.concat([df,pd.read_csv(path)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee7036b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clip_Name</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses02F_impro01_F000</td>\n",
       "      <td>Hi. Excuse me. Um, I'd like to put in this app...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses02F_impro01_F018</td>\n",
       "      <td>Well, why didn't the D.M.V. put that you neede...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses02F_impro01_F019</td>\n",
       "      <td>Yeah, but your birth certificate--I mean, who ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ses02F_impro01_F020</td>\n",
       "      <td>With your driver's license and your passport. ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ses02F_impro01_F021</td>\n",
       "      <td>Who--you always use your driver's license. I m...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Ses03M_script03_2_M040</td>\n",
       "      <td>Turn it--Turn it off.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Ses03M_script03_2_M041</td>\n",
       "      <td>Very amusing indeed.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Ses03M_script03_2_M042</td>\n",
       "      <td>You know what? You're a vile, little, evil-min...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Ses03M_script03_2_M043</td>\n",
       "      <td>You're not going nowhere. No you're not.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Ses03M_script03_2_M044</td>\n",
       "      <td>Just shut up.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Clip_Name  \\\n",
       "0       Ses02F_impro01_F000   \n",
       "1       Ses02F_impro01_F018   \n",
       "2       Ses02F_impro01_F019   \n",
       "3       Ses02F_impro01_F020   \n",
       "4       Ses02F_impro01_F021   \n",
       "..                      ...   \n",
       "580  Ses03M_script03_2_M040   \n",
       "581  Ses03M_script03_2_M041   \n",
       "582  Ses03M_script03_2_M042   \n",
       "583  Ses03M_script03_2_M043   \n",
       "584  Ses03M_script03_2_M044   \n",
       "\n",
       "                                                  text     Label    Use  \n",
       "0    Hi. Excuse me. Um, I'd like to put in this app...   Neutral  train  \n",
       "1    Well, why didn't the D.M.V. put that you neede...  Negative   test  \n",
       "2    Yeah, but your birth certificate--I mean, who ...  Negative  train  \n",
       "3    With your driver's license and your passport. ...  Negative  train  \n",
       "4    Who--you always use your driver's license. I m...  Negative  train  \n",
       "..                                                 ...       ...    ...  \n",
       "580                              Turn it--Turn it off.  Negative  train  \n",
       "581                               Very amusing indeed.  Negative  train  \n",
       "582  You know what? You're a vile, little, evil-min...  Negative  train  \n",
       "583           You're not going nowhere. No you're not.  Negative  train  \n",
       "584                                      Just shut up.  Negative   test  \n",
       "\n",
       "[1924 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Clip_Name\",\"text\",\"Label\",\"Use\"]]\n",
    "df = df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b792b528",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelEncoder = {\"Negative\" : 0, \"Neutral\" : 1, \"Positive\" : 2}\n",
    "df[\"Label\"] = [labelEncoder[label] for label in df[\"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45546469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clip_Name</th>\n",
       "      <th>text</th>\n",
       "      <th>Label</th>\n",
       "      <th>Use</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ses02F_impro01_F000</td>\n",
       "      <td>Hi. Excuse me. Um, I'd like to put in this app...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ses02F_impro01_F018</td>\n",
       "      <td>Well, why didn't the D.M.V. put that you neede...</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ses02F_impro01_F019</td>\n",
       "      <td>Yeah, but your birth certificate--I mean, who ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ses02F_impro01_F020</td>\n",
       "      <td>With your driver's license and your passport. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ses02F_impro01_F021</td>\n",
       "      <td>Who--you always use your driver's license. I m...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Ses03M_script03_2_M040</td>\n",
       "      <td>Turn it--Turn it off.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Ses03M_script03_2_M041</td>\n",
       "      <td>Very amusing indeed.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Ses03M_script03_2_M042</td>\n",
       "      <td>You know what? You're a vile, little, evil-min...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Ses03M_script03_2_M043</td>\n",
       "      <td>You're not going nowhere. No you're not.</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Ses03M_script03_2_M044</td>\n",
       "      <td>Just shut up.</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1924 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Clip_Name  \\\n",
       "0       Ses02F_impro01_F000   \n",
       "1       Ses02F_impro01_F018   \n",
       "2       Ses02F_impro01_F019   \n",
       "3       Ses02F_impro01_F020   \n",
       "4       Ses02F_impro01_F021   \n",
       "..                      ...   \n",
       "580  Ses03M_script03_2_M040   \n",
       "581  Ses03M_script03_2_M041   \n",
       "582  Ses03M_script03_2_M042   \n",
       "583  Ses03M_script03_2_M043   \n",
       "584  Ses03M_script03_2_M044   \n",
       "\n",
       "                                                  text  Label    Use  \n",
       "0    Hi. Excuse me. Um, I'd like to put in this app...      1  train  \n",
       "1    Well, why didn't the D.M.V. put that you neede...      0   test  \n",
       "2    Yeah, but your birth certificate--I mean, who ...      0  train  \n",
       "3    With your driver's license and your passport. ...      0  train  \n",
       "4    Who--you always use your driver's license. I m...      0  train  \n",
       "..                                                 ...    ...    ...  \n",
       "580                              Turn it--Turn it off.      0  train  \n",
       "581                               Very amusing indeed.      0  train  \n",
       "582  You know what? You're a vile, little, evil-min...      0  train  \n",
       "583           You're not going nowhere. No you're not.      0  train  \n",
       "584                                      Just shut up.      0   test  \n",
       "\n",
       "[1924 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e92f92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1080 Ti\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f45ab4dac00>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 19\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "else: \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ca1e073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6947d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "def bert_tokenization(df, maxLen):\n",
    "    sentences = df['text']\n",
    "    labels = df['Label'].tolist()\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\n",
    "    input_ids =  [tokenizer.encode(sent, add_special_tokens=True,max_length=maxLen,pad_to_max_length=True,truncation=True) for sent in sentences]\n",
    "    attention_mask = []\n",
    "    attention_masks = [[float(i>0) for i in seq] for seq in input_ids]\n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c66af7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    }
   ],
   "source": [
    "maxLen = 0\n",
    "\n",
    "for text in df['text']:\n",
    "    if len(text.split()) > maxLen:\n",
    "        maxLen = len(text.split())\n",
    "print(maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f1c4ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[df[\"Use\"] == \"train\"]\n",
    "val_data = df[df[\"Use\"] == \"validation\"]\n",
    "test_data = df[df[\"Use\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "853bf63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/MultiModal/venv/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_inputs, train_masks, train_labels = bert_tokenization(train_data,maxLen)\n",
    "val_inputs, val_masks, val_labels = bert_tokenization(val_data,maxLen)\n",
    "test_inputs, test_masks, test_labels = bert_tokenization(test_data,maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b122ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89efea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs,val_masks,val_labels)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d13c500f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef381dfb79444f08fe449608de595a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(device)\n",
    "\n",
    "# Parameters:\n",
    "lr = 2e-5\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "num_warmup_steps = 0\n",
    "num_training_steps = len(train_dataloader)*epochs\n",
    "\n",
    "### In Transformers, optimizer and schedules are splitted and instantiated like this:\n",
    "optimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8b785f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "# Import and evaluate each test batch using Matthew's correlation coefficient\n",
    "from sklearn.metrics import accuracy_score,matthews_corrcoef\n",
    "from tqdm import tqdm, trange,tnrange,tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdb14aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/park/Workspace/MultiModal/venv/lib/python3.6/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5799de80365a41ee9df3da4a50a2095f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<====================== Epoch 1 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  1e-05\n",
      "\n",
      "\tAverage Training loss: 0.7195086744991509\n",
      "\n",
      "\tValidation Accuracy: 0.6586174242424242\n",
      "\n",
      "\tValidation MCC Accuracy: 0.4321687470909374\n",
      "<====================== Epoch 2 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  5e-06\n",
      "\n",
      "\tAverage Training loss: 0.40955423463035273\n",
      "\n",
      "\tValidation Accuracy: 0.6839488636363636\n",
      "\n",
      "\tValidation MCC Accuracy: 0.47594014900969\n",
      "<====================== Epoch 3 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n",
      "\n",
      "\tAverage Training loss: 0.23337434010731206\n",
      "\n",
      "\tValidation Accuracy: 0.6647727272727273\n",
      "\n",
      "\tValidation MCC Accuracy: 0.43963233776496513\n",
      "<====================== Epoch 4 ======================>\n",
      "\n",
      "\tCurrent Learning rate:  0.0\n",
      "\n",
      "\tAverage Training loss: 0.19314471370465047\n",
      "\n",
      "\tValidation Accuracy: 0.6624053030303031\n",
      "\n",
      "\tValidation MCC Accuracy: 0.445162445352343\n"
     ]
    }
   ],
   "source": [
    "## Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "learning_rate = []\n",
    "\n",
    "# Gradients gets accumulated by default\n",
    "model.zero_grad()\n",
    "\n",
    "# tnrange is a tqdm wrapper around the normal python range\n",
    "for _ in tnrange(1,epochs+1,desc='Epoch'):\n",
    "  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n",
    "  # Calculate total loss for this epoch\n",
    "  batch_loss = 0\n",
    "\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Set our model to training mode (as opposed to evaluation mode)\n",
    "    model.train()\n",
    "    \n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip the norm of the gradients to 1.0\n",
    "    # Gradient clipping is not in AdamW anymore\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    \n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update learning rate schedule\n",
    "    scheduler.step()\n",
    "\n",
    "    # Clear the previous accumulated gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    batch_loss += loss.item()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_train_loss = batch_loss / len(train_dataloader)\n",
    "\n",
    "  #store the current learning rate\n",
    "  for param_group in optimizer.param_groups:\n",
    "    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n",
    "    learning_rate.append(param_group['lr'])\n",
    "    \n",
    "  train_loss_set.append(avg_train_loss)\n",
    "  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n",
    "    \n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables \n",
    "  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in val_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    \n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[0].to('cpu').numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    labels_flat = label_ids.flatten()\n",
    "    \n",
    "    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n",
    "    \n",
    "    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n",
    "    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n",
    "    \n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(F'\\n\\tValidation Accuracy: {eval_accuracy/nb_eval_steps}')\n",
    "  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy/nb_eval_steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ac23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
